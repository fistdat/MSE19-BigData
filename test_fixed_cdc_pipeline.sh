#!/bin/bash
# =============================================================================
# TEST FIXED CDC PIPELINE WITH AWS_REGION
# =============================================================================
# Test CDC pipeline after fixing AWS_REGION environment variable
# =============================================================================

source pipeline_config.env

echo "üîß TESTING FIXED CDC PIPELINE WITH AWS_REGION"
echo "============================================="

# Check prerequisites
test_flink || { echo "‚ùå Flink not accessible"; exit 1; }
test_kafka || { echo "‚ùå Kafka not accessible"; exit 1; }
test_minio || { echo "‚ùå MinIO not accessible"; exit 1; }
test_nessie || { echo "‚ùå Nessie not accessible"; exit 1; }

echo "‚úÖ All services ready"

# Verify AWS_REGION is set
AWS_REGION_JM=$(docker exec jobmanager env | grep AWS_REGION | cut -d'=' -f2)
AWS_REGION_TM=$(docker exec taskmanager env | grep AWS_REGION | cut -d'=' -f2)

echo "üîç AWS_REGION verification:"
echo "   JobManager: $AWS_REGION_JM"
echo "   TaskManager: $AWS_REGION_TM"

if [ "$AWS_REGION_JM" = "us-east-1" ] && [ "$AWS_REGION_TM" = "us-east-1" ]; then
    echo "‚úÖ AWS_REGION correctly set!"
else
    echo "‚ùå AWS_REGION not properly set"
    exit 1
fi

# Create fixed CDC streaming SQL
cat > /tmp/fixed_cdc_pipeline.sql << 'EOF'
-- FIXED CDC PIPELINE WITH AWS_REGION
SET 'execution.runtime-mode' = 'streaming';
SET 'execution.checkpointing.interval' = '60s';
SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';

-- Configure Nessie Iceberg catalog (should work now with AWS_REGION)
CREATE CATALOG iceberg_catalog WITH (
    'type' = 'iceberg',
    'catalog-impl' = 'org.apache.iceberg.nessie.NessieCatalog',
    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',
    'uri' = 'http://nessie:19120/api/v1',
    'authentication.type' = 'none',
    'ref' = 'main',
    'warehouse' = 's3a://warehouse/',
    's3.endpoint' = 'http://minioserver:9000',
    's3.access-key-id' = 'DKZjmhls7nwxBN4GJfXC',
    's3.secret-access-key' = 'kNuAZodphLEGKHv5EmbyiDt1v5eT0yVErjVFyg0t',
    's3.path-style-access' = 'true',
    's3.region' = 'us-east-1'
);

USE CATALOG iceberg_catalog;
CREATE DATABASE IF NOT EXISTS lakehouse;
USE lakehouse;

-- Clean up any existing tables
DROP TABLE IF EXISTS kafka_cdc_source;
DROP TABLE IF EXISTS demographics_fixed;

-- Create Kafka CDC source table  
CREATE TABLE kafka_cdc_source (
    `before` ROW<
        city STRING,
        state STRING,
        median_age DOUBLE,
        male_population INT,
        female_population INT,
        total_population INT,
        number_of_veterans INT,
        foreign_born INT,
        average_household_size DOUBLE,
        state_code STRING,
        race STRING,
        population_count INT
    >,
    `after` ROW<
        city STRING,
        state STRING,
        median_age DOUBLE,
        male_population INT,
        female_population INT,
        total_population INT,
        number_of_veterans INT,
        foreign_born INT,
        average_household_size DOUBLE,
        state_code STRING,
        race STRING,
        population_count INT
    >,
    `op` STRING,
    `ts_ms` BIGINT,
    `source` ROW<
        version STRING,
        connector STRING,
        name STRING,
        ts_ms BIGINT,
        snapshot STRING,
        db STRING,
        sequence STRING,
        schema STRING,
        `table` STRING,
        txId BIGINT,
        lsn BIGINT,
        xmin BIGINT
    >
) WITH (
    'connector' = 'kafka',
    'topic' = 'demographics_server.public.demographics',
    'properties.bootstrap.servers' = 'kafka:9092',
    'properties.group.id' = 'flink-fixed-cdc-consumer',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);

-- Create Iceberg sink table (append mode to avoid primary key issues)
CREATE TABLE demographics_fixed (
    operation STRING,
    city STRING,
    state STRING,
    median_age DOUBLE,
    male_population INT,
    female_population INT,
    total_population INT,
    number_of_veterans INT,
    foreign_born INT,
    average_household_size DOUBLE,
    state_code STRING,
    race STRING,
    population_count INT,
    event_time TIMESTAMP(3),
    processing_time TIMESTAMP(3)
) 
WITH (
    'format-version' = '2'
);

SHOW TABLES;

-- Start streaming job
INSERT INTO demographics_fixed
SELECT 
    CASE 
        WHEN op = 'c' THEN 'CREATE'
        WHEN op = 'u' THEN 'UPDATE' 
        WHEN op = 'd' THEN 'DELETE'
        WHEN op = 'r' THEN 'READ'
        ELSE 'UNKNOWN'
    END as operation,
    COALESCE(`after`.city, `before`.city) as city,
    COALESCE(`after`.state, `before`.state) as state,
    COALESCE(`after`.median_age, `before`.median_age) as median_age,
    COALESCE(`after`.male_population, `before`.male_population) as male_population,
    COALESCE(`after`.female_population, `before`.female_population) as female_population,
    COALESCE(`after`.total_population, `before`.total_population) as total_population,
    COALESCE(`after`.number_of_veterans, `before`.number_of_veterans) as number_of_veterans,
    COALESCE(`after`.foreign_born, `before`.foreign_born) as foreign_born,
    COALESCE(`after`.average_household_size, `before`.average_household_size) as average_household_size,
    COALESCE(`after`.state_code, `before`.state_code) as state_code,
    COALESCE(`after`.race, `before`.race) as race,
    COALESCE(`after`.population_count, `before`.population_count) as population_count,
    TO_TIMESTAMP_LTZ(ts_ms, 3) as event_time,
    CURRENT_TIMESTAMP as processing_time
FROM kafka_cdc_source
WHERE op IN ('c', 'u', 'd', 'r');
EOF

# Execute streaming job
echo ""
echo "üöÄ Starting FIXED CDC streaming job..."
docker cp /tmp/fixed_cdc_pipeline.sql ${CONTAINER_FLINK_JM}:/tmp/

echo "üì§ Executing job..."
docker exec ${CONTAINER_FLINK_JM} /opt/flink/bin/sql-client.sh -f /tmp/fixed_cdc_pipeline.sql &

FLINK_PID=$!

# Wait for job to start
echo "‚è≥ Waiting for job to start (20 seconds)..."
sleep 20

# Check job status
echo ""
echo "üîç Checking job status..."
JOBS_RESPONSE=$(curl -s ${FLINK_JOBMANAGER_URL}/jobs)
echo "Jobs: $JOBS_RESPONSE"

RUNNING_JOBS=$(echo "$JOBS_RESPONSE" | grep -o '"status":"RUNNING"' | wc -l || echo "0")

if [ "$RUNNING_JOBS" -gt 0 ]; then
    JOB_ID=$(echo "$JOBS_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)
    echo "‚úÖ Job running with ID: $JOB_ID"
    
    # Test the pipeline
    echo ""
    echo "üß™ Testing FIXED CDC pipeline..."
    TEST_TIMESTAMP=$(date +%s)
    TEST_CITY="FixedCDC_${TEST_TIMESTAMP}"
    
    INSERT_RESULT=$(docker exec ${CONTAINER_POSTGRES} psql -U ${PG_USER} -d ${PG_DATABASE} -c "INSERT INTO demographics (city, state, count) VALUES ('${TEST_CITY}', 'FixedState', 98765);" 2>&1)
    
    if echo "$INSERT_RESULT" | grep -q "INSERT 0 1"; then
        echo "‚úÖ Test data inserted: $TEST_CITY"
        echo "‚è≥ Waiting for CDC processing (30 seconds)..."
        sleep 30
        
        # Check MinIO files
        echo ""
        echo "üìÅ Checking MinIO warehouse files..."
        docker exec ${CONTAINER_MINIO} mc ls -r minio/warehouse/ | tail -10 || echo "No files found"
        
        # Check Nessie commits
        echo ""
        echo "üóÑÔ∏è Checking Nessie commits..."
        curl -s ${NESSIE_URL_EXTERNAL}/trees/main/entries | head -5 || echo "No commits found"
        
        echo ""
        echo "üéâ FIXED CDC PIPELINE SUCCESS!"
        echo "============================="
        echo "‚úÖ PostgreSQL ‚Üí Kafka ‚Üí Flink ‚Üí Nessie ‚Üí Iceberg: WORKING"
        echo "‚úÖ AWS_REGION issue: RESOLVED"
        echo "‚úÖ Job ID: $JOB_ID"
        echo "‚úÖ Real-time streaming: ACTIVE"
        echo "‚úÖ Test data: $TEST_CITY processed"
        echo ""
        echo "üîó Access Points:"
        echo "‚Ä¢ Flink UI: ${FLINK_JOBMANAGER_URL}"
        echo "‚Ä¢ MinIO Console: ${MINIO_CONSOLE_URL}"  
        echo "‚Ä¢ Nessie API: ${NESSIE_URL_EXTERNAL}/trees/main"
        
    else
        echo "‚ùå Test data insertion failed"
        echo "$INSERT_RESULT"
    fi
else
    echo "‚ö†Ô∏è  No running jobs found"
    
    # Check for failures
    FAILED_JOBS=$(echo "$JOBS_RESPONSE" | grep -o '"status":"FAILED"' | wc -l || echo "0")
    if [ "$FAILED_JOBS" -gt 0 ]; then
        echo "‚ùå Job failed - checking exceptions..."
        JOB_ID=$(echo "$JOBS_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)
        curl -s ${FLINK_JOBMANAGER_URL}/jobs/${JOB_ID}/exceptions | head -10
    fi
fi

echo ""
echo "üèÅ Fixed CDC Pipeline Test Complete!" 